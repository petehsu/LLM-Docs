# FAQ

> Найдите ответы на распространенные вопросы о MegaLLM - универсальной AI платформе, объединяющей более 70 моделей.

## Общие вопросы

<AccordionGroup>
  <Accordion title="Что такое MegaLLM?">
    MegaLLM - это универсальная AI платформа, которая объединяет более 70 больших языковых моделей от ведущих провайдеров, таких как OpenAI, Anthropic и Google, через единый API. Думайте о ней как о вашем "супер-API" для AI - вместо интеграции с несколькими провайдерами по отдельности, вы получаете доступ ко всем моделям через один унифицированный интерфейс.
  </Accordion>

  <Accordion title="Сколько моделей поддерживает MegaLLM?">
    В настоящее время мы поддерживаем более 70 моделей, включая:

    * **OpenAI**: gpt-5, gpt-4.1, gpt-4o, gpt-3.5-turbo
    * **Anthropic**: claude-opus-4-1-20250805, claude-sonnet-4, claude-3.5-sonnet, claude-3.7-sonnet
    * **Google**: gemini-2.5-pro, gemini-2.0-flash-001
    * **Модели эмбеддингов**: Различные опции для текстовых эмбеддингов

    Новые модели добавляются регулярно по мере их появления.
  </Accordion>

  <Accordion title="Могу ли я мгновенно переключаться между моделями?">
    Да! Переключение моделей так же просто, как изменение одного параметра в вашем API вызове. Вы также можете настроить автоматические резервные варианты между моделями.

    ```python  theme={null}
    # Мгновенное переключение моделей
    response = client.chat.completions.create(
        model="gpt-5",  # Измените это на любой поддерживаемый ID модели
        messages=[{"role": "user", "content": "Hello!"}]
    )
    ```
  </Accordion>

  <Accordion title="Нужны ли мне отдельные аккаунты для каждого AI провайдера?">
    Нет! В этом и заключается красота MegaLLM. Вам нужен только один аккаунт MegaLLM для доступа ко всем 70+ моделям. Мы берем на себя сложность управления отношениями с множеством провайдеров, так что вам не придется этим заниматься.
  </Accordion>
</AccordionGroup>

## Функции платформы

<AccordionGroup>
  <Accordion title="Чем MegaLLM отличается от прямого использования провайдеров?">
    MegaLLM предлагает несколько уникальных преимуществ:

    1. **Один API для всех**: Доступ к 70+ моделям через единый, консистентный интерфейс
    2. **Автоматические резервные варианты**: Если одна модель недоступна, автоматически переключается на другую
    3. **Единый биллинг**: Один счет за все ваше использование AI
    4. **Оптимизация производительности**: Интеллектуальная маршрутизация и балансировка нагрузки
    5. **Управление затратами**: Оптимизация расходов между различными моделями
  </Accordion>

  <Accordion title="Как работают автоматические резервные варианты?">
    Когда вы настраиваете резервные модели, MegaLLM автоматически перенаправляет ваш запрос к резервным моделям, если основная модель сталкивается с проблемами, такими как:

    * Лимиты запросов
    * Временные сбои
    * Ошибки таймаута
    * Ограничения мощности

    Это гарантирует, что ваше приложение никогда не остановится из-за отказа одной модели.

    ```python  theme={null}
    response = client.chat.completions.create(
        model="gpt-5",
        messages=messages,
        fallback_models=["claude-opus-4-1-20250805", "gemini-2.5-pro"]
    )
    ```
  </Accordion>
</AccordionGroup>

## Цены и биллинг

<AccordionGroup>
  <Accordion title="Как работает ценообразование?">
    Вы платите на основе фактического использования токенов, точно так же, как с отдельными провайдерами. Однако MegaLLM предлагает несколько преимуществ:

    * **Единый биллинг**: Один счет за все модели
    * **Скидки за объем**: Лучшие тарифы при высоком использовании
    * **Оптимизация затрат**: Инструменты для минимизации расходов
    * **Прозрачные цены**: Четкая разбивка стоимости по моделям

    См. нашу [страницу Моделей](/ru/home/models) для подробной информации о ценах.
  </Accordion>

  <Accordion title="Дороже ли MegaLLM, чем прямое использование провайдеров?">
    Для большинства пользователей MegaLLM предлагает лучшую ценность, потому что:

    1. **Объемные цены**: Мы передаем скидки за объем клиентам
    2. **Снижение затрат на разработку**: Не нужно интегрироваться с несколькими API
    3. **Операционная экономия**: Меньше мониторинга, меньше проблем с лимитами
    4. **Преимущества резервирования**: Более высокая доступность означает меньше потерянной выручки

    Кроме того, вы экономите значительное инженерное время, не управляя множественными интеграциями провайдеров.
  </Accordion>

  <Accordion title="Могу ли я установить лимиты расходов?">
    Да! MegaLLM предоставляет комплексные средства контроля затрат:

    * Дневные/месячные лимиты расходов
    * Распределение бюджета по моделям
    * Оповещения и уведомления об использовании
    * Рекомендации по оптимизации затрат
    * Автоматический переход к более дешевым моделям при достижении лимитов
  </Accordion>
</AccordionGroup>

## Техническая интеграция

<AccordionGroup>
  <Accordion title="Совместим ли MegaLLM с существующим кодом OpenAI?">
    Да! MegaLLM полностью совместим с форматом API OpenAI. Миграция обычно требует только изменения базового URL:

    ```python  theme={null}
    # До (OpenAI)
    client = OpenAI(api_key="sk-...")

    # После (MegaLLM)
    client = OpenAI(
        base_url="https://ai.megallm.io/v1",
        api_key="your-megallm-key"
    )
    ```

    Весь ваш существующий код продолжает работать без изменений.
  </Accordion>

  <Accordion title="А как насчет совместимости с Anthropic?">
    Мы также идеально поддерживаем формат API Anthropic:

    ```python  theme={null}
    # Формат Anthropic тоже работает
    client = Anthropic(
        base_url="https://ai.megallm.io",
        api_key="your-megallm-key"
    )
    ```

    Вы даже можете смешивать и сочетать - использовать формат OpenAI для доступа к моделям Claude или наоборот.
  </Accordion>

  <Accordion title="Как обрабатывать различные возможности моделей?">
    Разные модели имеют разные сильные стороны. MegaLLM упрощает маршрутизацию запросов к лучшей модели для каждой задачи:

    ```python  theme={null}
    # Генерация кода
    code_response = client.chat.completions.create(
        model="gpt-5",  # Отлично для кода
        messages=[{"role": "user", "content": "Write a Python function..."}]
    )

    # Креативное письмо
    creative_response = client.chat.completions.create(
        model="claude-opus-4-1-20250805",  # Отлично для креативных задач
        messages=[{"role": "user", "content": "Write a story..."}]
    )

    # Быстрые ответы
    quick_response = client.chat.completions.create(
        model="gpt-4o-mini",  # Быстро и эффективно
        messages=[{"role": "user", "content": "Quick question..."}]
    )
    ```
  </Accordion>

  <Accordion title="Как обстоят дела с лимитами запросов?">
    MegaLLM значительно снижает проблемы с лимитами запросов:

    1. **Распределенная нагрузка**: Запросы распределяются между несколькими провайдерами
    2. **Автоматические резервные варианты**: Переключение на доступные модели при достижении лимитов
    3. **Интеллектуальная маршрутизация**: Направление запросов к моделям с доступной мощностью
    4. **Предсказание лимитов**: Избегайте достижения лимитов с прогнозирующей маршрутизацией

    Вы получите значительно более высокие эффективные лимиты запросов, чем у любого отдельного провайдера.
  </Accordion>
</AccordionGroup>

## Сценарии использования

<AccordionGroup>
  <Accordion title="Кому больше всего подходит MegaLLM?">
    MegaLLM идеально подходит для:

    **Разработчиков**:

    * Экспериментируйте с различными моделями без переписывания кода
    * Снизьте сложность интеграции
    * Быстрее выводите продукты на рынок

    **Бизнеса**:

    * Обеспечьте высокую доступность с резервными вариантами
    * Оптимизируйте затраты между провайдерами
    * Защитите AI инвестиции для будущего

    **Исследователей**:

    * Получайте доступ к передовым моделям по мере их выпуска
    * Проводите комплексные оценки
    * Тестируйте производительность моделей на различных задачах
  </Accordion>

  <Accordion title="Могу ли я использовать MegaLLM для производственных приложений?">
    Абсолютно! MegaLLM разработан для производственного использования с:

    * SLA доступности 99.9%
    * Корпоративной безопасностью и соответствием стандартам
    * Мониторингом и поддержкой 24/7
    * Автоматическим масштабированием и балансировкой нагрузки
    * Комплексным логированием и аналитикой

    Многие компании используют MegaLLM для работы своих производственных AI функций.
  </Accordion>

  <Accordion title="Как выбрать правильную модель для моего сценария?">
    Ознакомьтесь с нашей [страницей Моделей](/ru/home/models) для подробного руководства. В общем:

    * **Быстрые ответы**: gpt-4o-mini, gemini-2.0-flash-001
    * **Сложные рассуждения**: gpt-5, claude-opus-4-1-20250805
    * **Генерация кода**: gpt-5, claude-3.7-sonnet
    * **Креативное письмо**: claude-opus-4-1-20250805, gpt-5
    * **Экономичность**: gpt-4o-mini, gemini-2.0-flash-001

    Вы также можете тестировать различные модели с вашими конкретными промптами, чтобы найти лучший вариант.
  </Accordion>
</AccordionGroup>

## Поддержка и начало работы

<AccordionGroup>
  <Accordion title="Как начать работу?">
    Начать работу просто:

    1. **Зарегистрируйтесь** для получения аккаунта MegaLLM
    2. **Получите ваш API ключ** из дашборда
    3. **Выберите метод интеграции** (формат OpenAI или Anthropic)
    4. **Сделайте ваш первый API вызов**

    Ознакомьтесь с нашим [руководством по быстрому старту](/ru/dev-docs/getting-started/quick-start) для подробных инструкций.
  </Accordion>

  <Accordion title="Предоставляете ли вы поддержку?">
    Да! Мы предоставляем:

    * **Документацию**: Комплексные руководства и туториалы
    * **Поддержку сообщества**: Сообщество Discord и форумы
    * **Поддержку по email**: Техническая помощь для всех пользователей
    * **Корпоративную поддержку**: Выделенная поддержка для корпоративных клиентов
    * **Профессиональные услуги**: Помощь с индивидуальной интеграцией

    Свяжитесь с нами по адресу [support@megallm.io](mailto:support@megallm.io) для любых вопросов.
  </Accordion>

  <Accordion title="Могу ли я мигрировать от моего текущего провайдера?">
    Миграция обычно очень проста, так как мы поддерживаем совместимость с API. Мы также предлагаем:

    * **Руководства по миграции** для популярных провайдеров
    * **Бесплатную помощь по миграции** для корпоративных клиентов
    * **Инструменты постепенной миграции** для тестирования перед полным переходом
    * **Инструменты сравнения затрат** для оптимизации вашей настройки

    Большинство клиентов могут мигрировать менее чем за час.
  </Accordion>
</AccordionGroup>

<Info>
  **Остались вопросы?** Ознакомьтесь с нашей [документацией](/ru/home/introduction) или обратитесь к нашей команде по адресу [support@megallm.io](mailto:support@megallm.io)
</Info>


---

> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.megallm.io/llms.txt