[Dashboard](/console/overview)  [Marketplace](/marketplace/index/agent)  [TrialCenter](/trialcenter/modeltrial/text)  [Documentation](//docs.bigmodel.cn/cn/guide/start/model-overview)  [Special Offer Zone¬†üî•](/special_area)

* Chinese
* English

 [API Login](/login?redirect=%2Fdev%2Fapi%2Fsuper-humanoid%2Femohaa)

GLM Model Fully Upgraded

Invite friends & Get rewards

Get up to 200M Tokens

![Â§ßÊ®°Âûã](https://cdn.bigmodel.cn/static/platform/images/logo/white_logo.png)

Try Zhipu‚Äôs New Flagship

GLM-4.6!

### Sign Up to Unlock AI capabilities

* Expert at coding, agents, reasoning, and more
* Get 20 millionfree Tokens on registration

Scan code

![Êô∫Ë∞±AI](https://cdn.bigmodel.cn/static/platform/images/activity/university/pop_right_bottom_new.png)

ÁªëÂÆöÊâãÊú∫Âè∑

Á°Æ ÂÆö

[Welcome](/dev/welcome)  [Guide](/dev/howuse)  [API Documentation](/dev/api)  [Guidelines](/dev/guidelines)  [ReleaseNotes](/dev/releasenotes)  [FAQs](/dev/faq)  [Model Benefit](/dev/activities) 

`‚åò``K`

API REFERENCE

* SDK Calling

  [+ Installation](/dev/api/devguide/sdk-install)

  [+ Authentication](/dev/api/devguide/sdk-auth)

  [+ sdk\_example](/dev/api/devguide/sdk_example)

* HTTP Request

  [+ API Request](/dev/api/http-call/http-para)

  [+ Authentication](/dev/api/http-call/http-auth)

* More Frameworks

  [+ OpenAI SDK](/dev/api/thirdparty-frame/openai-sdk)

  [+ Langchain SDK](/dev/api/thirdparty-frame/langchain-sdk)

APIs

* Language models

  [+ GLM-4 Models](/dev/api/normal-model/glm-4)

  [+ GLM-4V Models](/dev/api/normal-model/glm-4v)

* Reasoning models

  [+ GLM-Z1](/dev/api/Reasoning-models/glm-z1)

* Video Generation

  [+ CogVideoX](/dev/api/videomodel/cogvideox)

  [+ CogVideoX-3](/dev/api/videomodel/cogvideox-3)

  [+ Vidu Models](/dev/api/videomodel/vidu)

* Audio-Video

  [+ GLM-4-Voice](/dev/api/rtav/GLM-4-Voice)

  [+ GLM-Realtime](/dev/api/rtav/GLM-Realtime)

  [+ GLM-ASR](/dev/api/rtav/glm-asr)

* Reasoning models

  [+ GLM-4.1V-Thinking](/dev/api/visual-reasoning-model/GLM-4.1V-Thinking)

* Agent

  [+ TranslationAgent](/dev/api/agent/general_translation)

  [+ Professional Document Translation](/dev/api/agent/doc_translation_agent)

  [+ Social Science and Literary Translation](/dev/api/agent/social_literature_translation_agent)

  [+ Subtitle Translation for Film and Television](/dev/api/agent/subtitle_translation_agent)

  [+ Social Media Translation](/dev/api/agent/social_translation_agent)

  [+ AI Drawing](/dev/api/agent/ai_drawing_agent)

  [+ AI Comics](/dev/api/agent/cartoon_generator_agent)

  [+ Popular Special Effects Videos](/dev/api/agent/vidu_template_agent)

  [+ Resume and Job Matching Assistant](/dev/api/agent/job_matching_agent)

  [+ Customer Service Script Quality Inspection](/dev/api/agent/service_check_agent)

  [+ Sales Quality Inspection](/dev/api/agent/sales_check_agent)

  [+ Bill Recognition](/dev/api/agent/receipt_recognition_agent)

  [+ Clothes Recognition](/dev/api/agent/clothes_recognition_agent)

  [+ Contract Analysis](/dev/api/agent/contract_parser_agent)

  [+ Tendering Analysis Agent](/dev/api/agent/bidding_parser_agent)

  [+ Winning Bid Analysis Agent](/dev/api/agent/bidwin_parser_agent)

  [+ Intelligent Problem Solving](/dev/api/agent/intelligent_education_solve_agent)

  [+ Homework Grading](/dev/api/agent/intelligent_education_correction_agent)

* search-tool

  [+ Web Search API](/dev/api/search-tool/web-search)

  [+ Web Search in Chat](/dev/api/search-tool/websearch-in-chat)

  [+ Search Agent](/dev/api/search-tool/agent-search)

* Image Generation

  [+ CogView-4](/dev/api/image-model/cogview)

* Agent Model

  [+ GLM-4-AllTools](/dev/api/intelligent-agent-model/glm-4-alltools)

  [+ GLM-4-Assistant](/dev/api/intelligent-agent-model/assistantapi)

* Code Programming

  [+ CodeGeeX-4](/dev/api/code-model/codegeex-4)

* Embedding

  [+ Embedding](/dev/api/vector/embedding)

* Moderations

  [+ moderations](/dev/api/moderations/moderations)

* Role-playing

  [+ CharGLM-4](/dev/api/super-humanoid/charglm-4)

  [+ Emohaa](/dev/api/super-humanoid/emohaa)

* Agent Development Platform

  [+ „ÄêNew„Äëqingliuagent](/dev/api/Agent_Platform/newagent)

  [+ agent](/dev/api/Agent_Platform/agent)

  [+ qingliuSDK](/dev/api/Agent_Platform/agentsdk)

  [+ Knowledge](/dev/api/Agent_Platform/knowledge)

  [+ FinAgent](/dev/api/Agent_Platform/FinAgent)

* Batch

  [+ Batch](/dev/api/batch-api/batch)

* Data Management

  [+ File Management](/dev/api/knowlage-manage/queryfile)

  [+ File content extraction](/dev/api/knowlage-manage/queryextract)

  [+ Rerank](/dev/api/knowlage-manage/rerank)

* Error Codes

  [+ HTTP Status Codes](/dev/api/error-code/error-code-v4)

  [+ Model Error Codes](/dev/api/error-code/service-error)

More

[* Libraries](/dev/api/libraries)

[* API Pricing](/dev/api/product-billing)

[* Tokenizer](/dev/api/tokenizer)

[* Parameter Description](/dev/api/parameter-description)

[FAQ](//docs.bigmodel.cn/cn/faq) 

Customer Service

[Work Order](/ticket-submit) 

Consultation

[400-6883-991](tel:4006883991)

Weekdays 9:30-18:00

Help Center 

![ZHIPU¬∑AI](https://cdn.bigmodel.cn/static/platform/images/qr-code/technical_community.png)

##### Scan via Wechat

User Group

# Emohaa

Model Code: emohaa

Emohaa has studied the classic Hill Helping Theory, mastering the professional language skills of human psychotherapists. It possesses strong abilities in listening, emotional mirroring, and empathy, offering emotional support to users. Emohaa helps users understand their thoughts and feelings, learn to cope with emotional issues, and achieve an optimistic and positive mental and emotional state.Click to view [product pricing](https://www.bigmodel.cn/pricing) .

## **SSE Invocation**

**API Request**

| Transmission Method | https |
| --- | --- |
| Request URL | `https://open.bigmodel.cn/api/paas/v4/chat/completions` |
| Calling Method | Synchronous call, wait for the model to execute and return the final result or SSE call |
| Character Encoding | UTF-8 |
| Request Format | JSON |
| Response Format | JSON or Standard Stream Event |
| Request Type | POST |
| Development Language | Any development language capable of making HTTP requests |

### **Request Parameters**

| Parameter Name | Type | Required | Description |
| --- | --- | --- | --- |
| model | String | Yes | The model code to be called. |
| messages | List<Object> | Yes | When calling the language model, input the current conversation information list as prompts to the model in the form of a JSON array like `{"role": "user", "content": "Hello"}`. Possible message types include User message and Assistant message. |
| meta | Object | Yes | Role and user information data, including user\_info: user information, bot\_info: role information, bot\_name: role name, user\_name: user name. |
| user\_info | String | Yes | User information. |
| bot\_info | String | Yes | Role information. |
| bot\_name | String | Yes | Role name. |
| user\_name | String | Yes | User name. |
| request\_id | String | No | A unique identifier for each request, provided by the client side to distinguish each request. If not provided, the platform will generate it by default. |
| do\_sample | Boolean | No | When `do_sample` is true, the sampling strategy is enabled; when `do_sample` is false, the sampling strategy parameters such as `temperature`, `top_p` will not take effect. |
| stream | Boolean | No | Should be set to False or omitted for synchronous calls. If set to True, the model returns content in chunks through a standard Event Stream. The Event Stream ends with a `data: [DONE]` message. |
| temperature | Float | No | Sampling temperature, controlling the randomness of output. Must be a positive number. Valid range: (0.0, 1.0], cannot be 0. A higher value increases randomness and creativity; a lower value makes output more stable or predictable. Adjust according to application scenario. |
| top\_p | Float | No | Another method of sampling, called nucleus sampling. Valid range: (0.0, 1.0) open interval, cannot be 0 or 1, default is 0.7. The model considers results within the top\_p probability mass of tokens. For example, 0.1 means the decoder considers tokens from the top 10% of candidates. Adjust according to application scenario. |
| max\_tokens | Integer | No | The maximum number of tokens the model can output. |
| stop | List | No | The model stops generating when it encounters the specified stop character(s); currently only supports a single stop word, formatted as [‚Äústop\_word1‚Äù]. |
| user\_id | String | No | Unique ID of the end user, helping the platform to intervene in the end user‚Äôs illegal activities, generation of illegal and inappropriate information, or other abusive behavior. ID length requirement: at least 6 characters, maximum of 128 characters. Learn more |

### Messages Field Description

The model can accept two types of messages: User message and Assistant message. Each message type has a distinct format, as detailed below:

1. User message

| Parameter Name | Type | Required | Description |
| --- | --- | --- | --- |
| role | String | Yes | The role of the message, should be ‚Äúuser‚Äù in this case. |
| content | String | Yes | The content of the message. |

1. Assistant message

| Parameter Name | Type | Required | Description |
| --- | --- | --- | --- |
| role | String | Yes | The role of the message, should be ‚Äúassistant‚Äù in this case. |
| content | String | Yes | The content of the message. |

### Response Content

#### Synchronous Call Response Content

| Parameter Name | Type | Description |
| --- | --- | --- |
| id | String | Task ID |
| created | Long | Request creation time, in Unix timestamp seconds. |
| model | String | Model name |
| choices | List | Outputs of the current dialogue model |
| index | Integer | Result index |
| finish\_reason | String | Reason for model inference termination. ‚Äústop‚Äù for natural end or stop word triggered, ‚Äúlength‚Äù for reaching token length limit. |
| message | Object | Text message returned by the model |
| role | String | Role in the current dialogue, currently default to ‚Äúassistant‚Äù (model) |
| content | String | Content of the current dialogue |
| usage | Object | Token count statistics at the end of model call |
| prompt\_tokens | Integer | Number of tokens from user input |
| completion\_tokens | Integer | Number of tokens output by the model |
| total\_tokens | Integer | Total number of tokens |

#### Stream Response Content Block

| Parameter Name | Type | Description |
| --- | --- | --- |
| id | String | Task order number generated by Zhishang AI Open Platform, use this number when querying request results. |
| created | Long | Request creation time, in Unix timestamp seconds. |
| choices | List | Outputs of the current dialogue model |
| index | Integer | Result index |
| finish\_reason | String | Reason for model inference termination. ‚Äústop‚Äù for natural end or stop word triggered, ‚Äúlength‚Äù for reaching token length limit. |
| delta | Object | Incremental text returned by the model |
| role | String | Role in the current dialogue, currently default to ‚Äúassistant‚Äù (model) |
| content | String | Content of the current dialogue |
| usage | Object | Token count statistics at the end of model call |
| prompt\_tokens | Integer | Number of tokens from user input |
| completion\_tokens | Integer | Number of tokens output by the model |
| total\_tokens | Integer | Total number of tokens |

### Call Example

#### Synchronous Call

**Request Example**

```
from zhipuai import ZhipuAI    #You need to update the SDK to the latest version.
client = ZhipuAI(api_key="")  # Fill in your own APIKey
response = client.chat.completions.create(
    model="emohaa",  # Fill in the model name to call
      meta: {
        "user_info": "A 30-year-old male software engineer with interests in reading, hiking, and programming.",
        "bot_info": "Emohaa is an emotional support AI based on the Hill Helping Theory, with professional psychotherapeutic language skills.",
        "bot_name": "Emohaa",
        "user_name": "Zhang San"
    },
     messages: [
        {
            "role": "assistant",
            "content": "Hello, I'm Emohaa. It's nice to meet you. How can I assist you today?"
        },
        {
            "role": "user",
            "content": "I've been feeling a lot of stress lately, and my mood is always low."
        },
        {
            "role": "assistant",
            "content": "It sounds like you've been facing quite a few challenges recently. Could you tell me more specifically what's been causing you stress?"
        },
        {
            "role": "user",
            "content": "It's mainly the pressure from work. There are too many tasks, and I always feel like I can't finish them."
        }
    ]
)
print(response.choices[0].message)
```

1  
2  
3  
4  
5  
6  
7  
8  
9  
10  
11  
12  
13  
14  
15  
16  
17  
18  
19  
20  
21  
22  
23  
24  
25  
26  
27  
28  
29  
30

**Response Example**

```
{
    "created": 1703487403,
    "id": "8239375684858666781",
    "model": "emohaa",
    "request_id": "8239375684858666781",
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": " I understand how you feel. Having a heavy workload can indeed be overwhelming. Have you tried any relaxation methods, such as taking breaks, exercising, or talking with friends?n",
                "role": "assistant"
            }
        }
    ],
    "usage": {
    "completion_tokens": 217,
    "prompt_tokens": 31,
    "total_tokens": 248
    }
}
```

1  
2  
3  
4  
5  
6  
7  
8  
9  
10  
11  
12  
13  
14  
15  
16  
17  
18  
19  
20  
21

#### Streaming Call

Request Example

```
from zhipuai import ZhipuAI   #You need to update the SDK to the latest version.
client = ZhipuAI(api_key="")  # Please fill in your own APIKey
response = client.chat.completions.create(
    model="emohaa",  # Fill in the model name to call
      meta: {
        "user_info": "A 30-year-old male software engineer with interests in reading, hiking, and programming.",
        "bot_info": "Emohaa is an emotional support AI based on the Hill Helping Theory, with professional psychotherapeutic language skills.",
        "bot_name": "Emohaa",
        "user_name": "Zhang San"
    },
     messages: [
        {
            "role": "assistant",
            "content": "Hello, I'm Emohaa. It's nice to meet you. How can I assist you today?"
        },
        {
            "role": "user",
            "content": "I've been feeling a lot of stress lately, and my mood is always low."
        },
        {
            "role": "assistant",
            "content": "It sounds like you've been facing quite a few challenges recently. Could you tell me more specifically what's been causing you stress?"
        },
        {
            "role": "user",
            "content": "It's mainly the pressure from work. There are too many tasks, and I always feel like I can't finish them."
        }
    ],
    stream=True,
)
for chunk in response:
    print(chunk.choices[0].delta)
```

1  
2  
3  
4  
5  
6  
7  
8  
9  
10  
11  
12  
13  
14  
15  
16  
17  
18  
19  
20  
21  
22  
23  
24  
25  
26  
27  
28  
29  
30  
31  
32

**Response Example**

```
{"id": "12345", "created": 1703488723, "model": "emohaa", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "\nI"}}]}
{"id": "12345", "created": 1703488723, "model": "emohaa", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "Ôºåunderstand "}}]}
{"id": "12345", "created": 1703488723, "model": "emohaa", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "how"}}]}

... ...

{"id": "12345", "created": 1703488723, "model": "emohaa", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "with"}}]}
{"id": "12345", "created": 1703488723, "model": "emohaa", "choices": [{"index": 0, "delta": {"role": "assistant", "content": " friends"}}]}
{"id": "12345", "created": 1703488723, "model": "emohaa", "choices": [{"index": 0, "finish_reason": "stop", "delta": {"role": "assistant", "content": ""}}], "usage": {"prompt_tokens": 24, "completion_tokens": 14, "total_tokens": 38}}
```

1  
2  
3  
4  
5  
6  
7  
8  
9

## Asynchronous API

**API Request**

| Transmission Method | HTTPS |
| --- | --- |
| Request URL | `https://open.bigmodel.cn/api/paas/v4/async/chat/completions` |
| Calling Method | Asynchronous, results must be fetched using a query interface |
| Character Encoding | UTF-8 |
| Request Format | JSON |
| Response Format | JSON |
| HTTP Method | POST |
| Development Language | Any capable of making HTTP requests |

### Request Parameters

The request parameters are the same as those for synchronous API calls.

### Response Parameters

| Parameter Name | Type | Description |
| --- | --- | --- |
| request\_id | String | The task number submitted by the client or generated by the platform during the request initiation. |
| id | String | Task order number generated by Zhishang AI Open Platform, used when querying results. |
| model | String | The name of the model called during the API request. |
| task\_status | string | Processing status of the request: `PROCESSING` (in progress), `SUCCESS` (successful), `FAIL` (failed). This status must be queried to determine the outcome. |

### Call Example

**Request Example**

```
from zhipuai import ZhipuAI  #You need to update the SDK to the latest version.

client = ZhipuAI(api_key="") # Please fill in your own API Key
response = client.chat.asyncCompletions.create(
    model="emohaa",  # The model to be used.
     meta: {
        "user_info": "A 30-year-old male software engineer with interests in reading, hiking, and programming.",
        "bot_info": "Emohaa is an emotional support AI based on the Hill Helping Theory, with professional psychotherapeutic language skills.",
        "bot_name": "Emohaa",
        "user_name": "Zhang San"
    },
     messages: [
        {
            "role": "assistant",
            "content": "Hello, I'm Emohaa. It's nice to meet you. How can I assist you today?"
        },
        {
            "role": "user",
            "content": "I've been feeling a lot of stress lately, and my mood is always low."
        },
        {
            "role": "assistant",
            "content": "It sounds like you've been facing quite a few challenges recently. Could you tell me more specifically what's been causing you stress?"
        },
        {
            "role": "user",
            "content": "It's mainly the pressure from work. There are too many tasks, and I always feel like I can't finish them."
        }
    ]
)

print(response)
```

1  
2  
3  
4  
5  
6  
7  
8  
9  
10  
11  
12  
13  
14  
15  
16  
17  
18  
19  
20  
21  
22  
23  
24  
25  
26  
27  
28  
29  
30  
31  
32

**Response Example**

```
id='123456789' request_id='654321' model='emohaa' task_status='PROCESSING'
```

1

## Task Result Query

#### Interface Request

| Transmission Method | https |
| --- | --- |
| Request URL | https://open.bigmodel.cn/api/paas/v4/async-result/{id} |
| Calling Method | Synchronous invocation, wait for the model to execute completely and return the final result |
| Character Encoding | UTF-8 |
| Request Format | JSON |
| Response Format | JSON |
| HTTP Method | GET |
| Development Language | Any development language capable of initiating an HTTP request |

### Request Parameters

| Parameter Name | Type | Required | Description |
| --- | --- | --- | --- |
| id | String | Yes | Task id |

### Response Parameters

| Parameter Name | Type | Description |
| --- | --- | --- |
| model | String | Model name |
| choices | List | Current dialogue model output content, currently only returns one |
| index | Integer | Result index |
| finish\_reason | String | Reason for model inference termination. ‚Äústop‚Äù for natural end or triggering stop words, ‚Äúlength‚Äù for reaching token length limit. |
| message | Object | Model returned text message |
| role | String | Role in the current dialogue, currently defaults to assistant (model) |
| content | String | Content of the current dialogue |
| task\_status | String | Processing status: PROCESSING (in process), SUCCESS (successful), FAIL (failed) |
| request\_id | String | Task number submitted by the user during client request or generated by the platform |
| id | String | Task order number generated by Zhipu AI Open Platform, use this order number when calling the request result interface |
| usage | Object | Token count statistics for this model invocation |
| prompt\_tokens | int | Number of tokens input by the user |
| completion\_tokens | int | Number of tokens output by the model |
| total\_tokens | int | Total number of tokens |

### Call Example

Request Example:

```
import time
from zhipuai import ZhipuAI   #You need to update the SDK to the latest version.

client = ZhipuAI(api_key="")  # Please fill in your own API Key

response = client.chat.asyncCompletions.create(
    model="emohaa",  # Enter the model name to be used
    meta: {
        "user_info": "A 30-year-old male software engineer with interests in reading, hiking, and programming.",
        "bot_info": "Emohaa is an emotional support AI based on the Hill Helping Theory, with professional psychotherapeutic language skills.",
        "bot_name": "Emohaa",
        "user_name": "Zhang San"
    },
     messages: [
        {
            "role": "assistant",
            "content": "Hello, I'm Emohaa. It's nice to meet you. How can I assist you today?"
        },
        {
            "role": "user",
            "content": "I've been feeling a lot of stress lately, and my mood is always low."
        },
        {
            "role": "assistant",
            "content": "It sounds like you've been facing quite a few challenges recently. Could you tell me more specifically what's been causing you stress?"
        },
        {
            "role": "user",
            "content": "It's mainly the pressure from work. There are too many tasks, and I always feel like I can't finish them."
        }
    ]
)
task_id = response.id
task_status = ''
get_cnt = 0

while task_status != 'SUCCESS' and task_status != 'FAILED' and get_cnt <= 40:
    result_response = client.chat.asyncCompletions.retrieve_completion_result(id=task_id)
    print(result_response)
    task_status = result_response.task_status

    time.sleep(2)
    get_cnt += 1
```

1  
2  
3  
4  
5  
6  
7  
8  
9  
10  
11  
12  
13  
14  
15  
16  
17  
18  
19  
20  
21  
22  
23  
24  
25  
26  
27  
28  
29  
30  
31  
32  
33  
34  
35  
36  
37  
38  
39  
40  
41  
42  
43

**Response Example:**

```
{"id":"123456789","request_id":"123123123","model":null,"task_status":"PROCESSING"}
{"id":"123456789","request_id":"123123123","model":null,"task_status":"PROCESSING"}

... ...

{"id":"123456789","request_id":"123123123","model":null,"task_status":"PROCESSING"}
{
    "id": "123456789",
    "request_id": "123123123",
    "model": "emohaa",
    "task_status": "SUCCESS",
    "choices": [
        {
            "index": 0,
            "finish_reason": "stop",
            "message": {
                "content": "I understand how you feel. Having a heavy workload can indeed be overwhelming. Have you tried any relaxation methods, such as taking breaks, exercising, or talking with friends?",
                "role": "assistant"
            }
        }
    ],
    "usage": {
        "prompt_tokens": 52,
        "completion_tokens": 470,
        "total_tokens": 522
    }
}
```

1  
2  
3  
4  
5  
6  
7  
8  
9  
10  
11  
12  
13  
14  
15  
16  
17  
18  
19  
20  
21  
22  
23  
24  
25  
26  
27

Table of contents

SSE Invocation

Request Parameters

Messages Field Description

Response Content

Call Example

Asynchronous API

Request Parameters

Response Parameters

Call Example

Task Result Query

Request Parameters

Response Parameters

Call Example