#!/usr/bin/env python3
"""
æ™ºè°± BigModel è‹±æ–‡ç«™æ–‡æ¡£ä¸‹è½½å·¥å…· (open.bigmodel.cn)

è¿™æ˜¯ Vue SPA åº”ç”¨ï¼Œéœ€è¦ç”¨ Playwright æ¸²æŸ“é¡µé¢åæå–å†…å®¹
"""

import sys
import time
import re
import asyncio
from pathlib import Path
from playwright.async_api import async_playwright
from markdownify import markdownify as md

sys.stdout.reconfigure(line_buffering=True)

OUTPUT_BASE = "BigModel Zhipu/English"

# å…¥å£é¡µé¢ï¼Œç”¨äºçˆ¬å–ä¾§è¾¹æ é“¾æ¥
ENTRY_PAGES = [
    "/dev/api/normal-model/glm-4",
    "/dev/api/thirdparty/overview",
    "/dev/howuse/introduction",
    "/dev/api/devguide/model",
]

async def get_all_links(page):
    """ä»é¡µé¢ä¾§è¾¹æ è·å–æ‰€æœ‰æ–‡æ¡£é“¾æ¥"""
    all_links = set()
    
    for entry in ENTRY_PAGES:
        url = f"https://open.bigmodel.cn{entry}"
        print(f"  çˆ¬å– {entry}...")
        
        try:
            await page.goto(url, wait_until="networkidle", timeout=30000)
            await page.wait_for_timeout(2000)
            
            # æå–ä¾§è¾¹æ é“¾æ¥
            links = await page.evaluate('''() => {
                const links = [];
                document.querySelectorAll('a[href^="/dev/"]').forEach(a => {
                    const href = a.getAttribute('href');
                    if (href && !href.includes('#')) {
                        links.push(href);
                    }
                });
                return links;
            }''')
            
            all_links.update(links)
            print(f"    å‘ç° {len(links)} ä¸ªé“¾æ¥")
        except Exception as e:
            print(f"    å¤±è´¥: {e}")
    
    return sorted(all_links)


async def download_page(page, path):
    """ä¸‹è½½å•ä¸ªé¡µé¢"""
    url = f"https://open.bigmodel.cn{path}"
    
    # è¾“å‡ºè·¯å¾„
    rel_path = path.replace('/dev/', '', 1)
    output = f"{OUTPUT_BASE}/{rel_path}.md"
    
    # è·³è¿‡å·²å­˜åœ¨
    if Path(output).exists():
        return True, "è·³è¿‡"
    
    try:
        await page.goto(url, wait_until="networkidle", timeout=30000)
        await page.wait_for_timeout(1500)
        
        # æå–ä¸»å†…å®¹åŒºåŸŸ
        content_html = await page.evaluate('''() => {
            // å°è¯•å¤šç§é€‰æ‹©å™¨
            const selectors = [
                '.markdown-body',
                '.doc-content',
                '.content-wrapper',
                'article',
                'main',
                '.main-content'
            ];
            
            for (const sel of selectors) {
                const el = document.querySelector(sel);
                if (el && el.innerText.length > 100) {
                    return el.innerHTML;
                }
            }
            
            // å›é€€ï¼šè·å–æ•´ä¸ªé¡µé¢å†…å®¹åŒº
            const main = document.querySelector('#app');
            return main ? main.innerHTML : document.body.innerHTML;
        }''')
        
        if not content_html or len(content_html) < 100:
            return False, "å†…å®¹ä¸ºç©º"
        
        # è½¬æ¢ä¸º Markdown
        markdown = md(content_html, heading_style="ATX", strip=['script', 'style', 'nav'])
        
        # æ¸…ç†
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)
        markdown = markdown.strip()
        
        if len(markdown) < 50:
            return False, "è½¬æ¢åå†…å®¹å¤ªçŸ­"
        
        Path(output).parent.mkdir(parents=True, exist_ok=True)
        with open(output, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        return True, "OK"
    except Exception as e:
        return False, str(e)[:50]


async def main():
    print("=" * 50)
    print("æ™ºè°± BigModel è‹±æ–‡ç«™æ–‡æ¡£ä¸‹è½½å·¥å…·")
    print("=" * 50)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy={"server": "http://127.0.0.1:10808"}
        )
        page = await browser.new_page()
        
        # å°è¯•ä»æ–‡ä»¶è¯»å–é“¾æ¥ï¼Œå¦åˆ™çˆ¬å–
        links_file = Path('zhipu_en_links.txt')
        if links_file.exists():
            print("\nğŸ“‹ ä»æ–‡ä»¶è¯»å–é“¾æ¥...")
            docs = links_file.read_text().strip().split('\n')
        else:
            print("\nğŸ“‹ çˆ¬å–æ–‡æ¡£é“¾æ¥...")
            docs = await get_all_links(page)
            if docs:
                with open('zhipu_en_links.txt', 'w') as f:
                    f.write('\n'.join(docs))
        
        if not docs:
            print("æœªæ‰¾åˆ°æ–‡æ¡£é“¾æ¥")
            await browser.close()
            return
        
        print(f"   å…± {len(docs)} ä¸ªé“¾æ¥")
        
        # ä¸‹è½½
        print(f"\nğŸ“š å¼€å§‹ä¸‹è½½...")
        success, skipped, fail = 0, 0, 0
        failed_docs = []
        
        for path in docs:
            name = path.split('/')[-1]
            ok, status = await download_page(page, path)
            
            if ok:
                if status == "è·³è¿‡":
                    skipped += 1
                else:
                    success += 1
                    print(f"    âœ“ {name}")
            else:
                fail += 1
                failed_docs.append((path, status))
                print(f"    âœ— {name} ({status})")
            
            if status != "è·³è¿‡":
                await page.wait_for_timeout(500)
        
        await browser.close()
        
        print(f"\nâœ… å®Œæˆ: {success} æ–°ä¸‹è½½, {skipped} è·³è¿‡, {fail} å¤±è´¥")
        
        if failed_docs:
            print("\nâŒ å¤±è´¥åˆ—è¡¨:")
            for path, status in failed_docs[:10]:
                print(f"    {path} - {status}")


if __name__ == "__main__":
    asyncio.run(main())
