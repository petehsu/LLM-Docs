#!/usr/bin/env python3
"""
è‡ªåŠ¨çˆ¬å–è°ƒåº¦å™¨ - å®šæ—¶çˆ¬å–å„å®¶ LLM API æ–‡æ¡£
åŒ…å«è¯¦ç»†çš„äº‹ä»¶æ—¥å¿—ç³»ç»Ÿ
"""

import os
import sys
import json
import subprocess
import time
import traceback
import re
from datetime import datetime
from pathlib import Path

# çˆ¬è™«é…ç½®
CRAWLERS = [
    {
        'id': 'grok',
        'name': 'xAI Grok',
        'script': 'download_grok.py',
        'auto': True,
        'interval': 86400,
    },
    {
        'id': 'megallm',
        'name': 'MegaLLM',
        'script': 'download_megallm.py',
        'auto': True,
        'interval': 86400,
    },
    {
        'id': 'minimax',
        'name': 'MiniMax',
        'script': 'download_minimax.py',
        'auto': True,
        'interval': 86400,
    },
    {
        'id': 'zhipu',
        'name': 'Zhipu BigModel (CN)',
        'script': 'download_zhipu.py',
        'auto': True,
        'interval': 86400,
    },
    {
        'id': 'claude',
        'name': 'Anthropic Claude',
        'script': 'batch_download_docs.py',
        'auto': True,
        'interval': 86400,
        'args': ['--claude-only'],
    },
    {
        'id': 'gemini',
        'name': 'Google Gemini',
        'script': 'batch_download_docs.py',
        'auto': True,
        'interval': 86400,
        'args': ['--gemini-only'],
    },
    {
        'id': 'openai',
        'name': 'OpenAI',
        'script': 'download_openai_uc.py',
        'auto': False,
        'interval': 86400,
    },
    {
        'id': 'moonshot',
        'name': 'Moonshot Kimi',
        'script': 'download_moonshot.py',
        'auto': False,
        'interval': 86400,
    },
    {
        'id': 'zhipu_en',
        'name': 'Zhipu BigModel (EN)',
        'script': 'download_zhipu_en.py',
        'auto': False,
        'interval': 86400,
    },
    {
        'id': 'meta',
        'name': 'Meta Llama',
        'script': 'download_meta.py',
        'auto': False,
        'interval': 86400,
    },
    {
        'id': 'deepseek',
        'name': 'DeepSeek',
        'script': 'download_deepseek.py',
        'auto': False,
        'interval': 86400,
    },
]

STATUS_FILE = 'data/crawl-status.json'
EVENTS_FILE = 'data/crawl-events.json'
DOCS_DIR = 'data/docs'
MAX_EVENTS = 500  # æœ€å¤šä¿ç•™çš„äº‹ä»¶æ•°é‡

# çˆ¬å–æ—¶é—´æˆ³æ ‡è®°ï¼ˆå¤šè¯­è¨€ï¼‰
CRAWL_HEADER_MARKERS = {
    'en': '> ğŸ“„ *Auto-crawled by [LLM Docs](https://petehsu.github.io/LLM-Docs/) on {date}*\n\n',
    'zh': '> ğŸ“„ *ç”± [LLM Docs](https://petehsu.github.io/LLM-Docs/) è‡ªåŠ¨çˆ¬å–äº {date}*\n\n',
    'ja': '> ğŸ“„ *[LLM Docs](https://petehsu.github.io/LLM-Docs/) ã«ã‚ˆã‚Š {date} ã«è‡ªå‹•å–å¾—*\n\n',
}

# æ£€æµ‹è¯­è¨€çš„å…³é”®è¯
LANG_DETECT_PATTERNS = {
    'zh': [r'[\u4e00-\u9fff]'],  # ä¸­æ–‡å­—ç¬¦
    'ja': [r'[\u3040-\u309f\u30a0-\u30ff]'],  # æ—¥æ–‡å‡å
}


def detect_doc_language(content, filepath):
    """æ£€æµ‹æ–‡æ¡£è¯­è¨€"""
    # å…ˆä»è·¯å¾„åˆ¤æ–­
    path_lower = filepath.lower()
    if '/chinese/' in path_lower or '/zh/' in path_lower or 'ä¸­æ–‡' in filepath:
        return 'zh'
    if '/japanese/' in path_lower or '/ja/' in path_lower or 'æ—¥æœ¬èª' in filepath:
        return 'ja'
    if '/english/' in path_lower or '/en/' in path_lower:
        return 'en'
    
    # ä»å†…å®¹åˆ¤æ–­
    sample = content[:1000]
    
    # æ£€æµ‹æ—¥æ–‡ï¼ˆå…ˆæ£€æµ‹ï¼Œå› ä¸ºæ—¥æ–‡ä¹Ÿå¯èƒ½åŒ…å«æ±‰å­—ï¼‰
    for pattern in LANG_DETECT_PATTERNS['ja']:
        if re.search(pattern, sample):
            return 'ja'
    
    # æ£€æµ‹ä¸­æ–‡
    for pattern in LANG_DETECT_PATTERNS['zh']:
        if re.search(pattern, sample):
            return 'zh'
    
    return 'en'


def has_crawl_header(content):
    """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²æœ‰çˆ¬å–æ—¶é—´æˆ³"""
    for marker in CRAWL_HEADER_MARKERS.values():
        # æ£€æŸ¥æ˜¯å¦åŒ…å« LLM Docs æ ‡è®°
        if 'Auto-crawled by [LLM Docs]' in content or 'ç”± [LLM Docs]' in content or '[LLM Docs]' in content[:500]:
            return True
    return False


def add_crawl_header(filepath, crawl_time=None):
    """ä¸ºæ–‡æ¡£æ·»åŠ çˆ¬å–æ—¶é—´æˆ³"""
    if crawl_time is None:
        crawl_time = datetime.now()
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # å·²æœ‰æ ‡è®°åˆ™è·³è¿‡
        if has_crawl_header(content):
            return False
        
        # æ£€æµ‹è¯­è¨€
        lang = detect_doc_language(content, filepath)
        
        # æ ¼å¼åŒ–æ—¥æœŸ
        date_str = crawl_time.strftime('%Y-%m-%d %H:%M UTC')
        
        # è·å–å¯¹åº”è¯­è¨€çš„æ ‡è®°
        header = CRAWL_HEADER_MARKERS.get(lang, CRAWL_HEADER_MARKERS['en'])
        header = header.format(date=date_str)
        
        # æ·»åŠ åˆ°æ–‡æ¡£å¼€å¤´
        new_content = header + content
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        return True
    except Exception as e:
        print(f"  âš ï¸ Failed to add header to {filepath}: {e}")
        return False


def add_headers_to_new_docs(vendor_id, crawl_time):
    """ä¸ºæ–°çˆ¬å–çš„æ–‡æ¡£æ·»åŠ æ—¶é—´æˆ³"""
    vendor_docs_dir = None
    
    # æ˜ å°„ vendor_id åˆ°æ–‡æ¡£ç›®å½•
    vendor_dir_map = {
        'grok': 'X Grok',
        'megallm': 'MegaLLM',
        'minimax': 'MiniMax',
        'zhipu': 'BigModel Zhipu',
        'claude': 'Anthropic Claude',
        'gemini': 'Google Gemini',
        'openai': 'OpenAI',
        'moonshot': 'Moonshot Kimi',
        'zhipu_en': 'BigModel Zhipu',
        'meta': 'Meta Llama',
        'deepseek': 'DeepSeek',
    }
    
    dir_name = vendor_dir_map.get(vendor_id)
    if dir_name:
        vendor_docs_dir = os.path.join(DOCS_DIR, dir_name)
    
    if not vendor_docs_dir or not os.path.exists(vendor_docs_dir):
        return 0
    
    added_count = 0
    for root, dirs, files in os.walk(vendor_docs_dir):
        for file in files:
            if file.endswith('.md'):
                filepath = os.path.join(root, file)
                if add_crawl_header(filepath, crawl_time):
                    added_count += 1
    
    return added_count


# ============ äº‹ä»¶æ—¥å¿—ç³»ç»Ÿ ============

class EventType:
    INFO = 'info'
    SUCCESS = 'success'
    WARNING = 'warning'
    ERROR = 'error'
    START = 'start'
    COMPLETE = 'complete'
    SKIP = 'skip'


def load_events():
    """åŠ è½½äº‹ä»¶æ—¥å¿—"""
    if os.path.exists(EVENTS_FILE):
        try:
            with open(EVENTS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []


def save_events(events):
    """ä¿å­˜äº‹ä»¶æ—¥å¿—"""
    os.makedirs(os.path.dirname(EVENTS_FILE), exist_ok=True)
    # åªä¿ç•™æœ€è¿‘çš„äº‹ä»¶
    events = events[-MAX_EVENTS:]
    with open(EVENTS_FILE, 'w', encoding='utf-8') as f:
        json.dump(events, f, ensure_ascii=False, indent=2)


def log_event(event_type, vendor_id, vendor_name, message, details=None):
    """è®°å½•äº‹ä»¶"""
    events = load_events()
    
    event = {
        'id': len(events) + 1,
        'timestamp': datetime.now().isoformat(),
        'type': event_type,
        'vendorId': vendor_id,
        'vendorName': vendor_name,
        'message': message,
        'details': details,
    }
    
    events.append(event)
    save_events(events)
    
    # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
    icon_map = {
        EventType.INFO: 'â„¹ï¸',
        EventType.SUCCESS: 'âœ…',
        EventType.WARNING: 'âš ï¸',
        EventType.ERROR: 'âŒ',
        EventType.START: 'ğŸš€',
        EventType.COMPLETE: 'ğŸ‰',
        EventType.SKIP: 'â­ï¸',
    }
    icon = icon_map.get(event_type, 'â€¢')
    print(f"  {icon} [{vendor_name}] {message}")
    
    return event


def log_system_event(event_type, message, details=None):
    """è®°å½•ç³»ç»Ÿäº‹ä»¶"""
    return log_event(event_type, 'system', 'System', message, details)


# ============ çŠ¶æ€ç®¡ç† ============

def load_status():
    """åŠ è½½çˆ¬å–çŠ¶æ€"""
    if os.path.exists(STATUS_FILE):
        with open(STATUS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_status(status):
    """ä¿å­˜çˆ¬å–çŠ¶æ€"""
    os.makedirs(os.path.dirname(STATUS_FILE), exist_ok=True)
    with open(STATUS_FILE, 'w', encoding='utf-8') as f:
        json.dump(status, f, ensure_ascii=False, indent=2)


# ============ çˆ¬è™«æ‰§è¡Œ ============

def run_crawler(crawler):
    """è¿è¡Œå•ä¸ªçˆ¬è™«"""
    script = crawler['script']
    args = crawler.get('args', [])
    crawler_id = crawler['id']
    crawler_name = crawler['name']
    
    if not os.path.exists(script):
        log_event(EventType.ERROR, crawler_id, crawler_name, 
                  f"Script not found: {script}")
        return False, "Script not found", 0, 0
    
    log_event(EventType.START, crawler_id, crawler_name,
              f"Starting crawler: {script} {' '.join(args)}")
    
    start_time = time.time()
    crawl_time = datetime.now()
    
    try:
        cmd = [sys.executable, '-u', script] + args
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600
        )
        
        duration = round(time.time() - start_time, 2)
        
        # è§£æè¾“å‡ºè·å–æ–‡æ¡£æ•°é‡
        doc_count = 0
        output_lines = result.stdout.split('\n') if result.stdout else []
        for line in output_lines:
            if 'docs' in line.lower() or 'æ–‡æ¡£' in line:
                # å°è¯•æå–æ•°å­—
                numbers = re.findall(r'\d+', line)
                if numbers:
                    doc_count = max(doc_count, int(numbers[0]))
        
        if result.returncode == 0:
            # ä¸ºæ–°çˆ¬å–çš„æ–‡æ¡£æ·»åŠ æ—¶é—´æˆ³
            headers_added = add_headers_to_new_docs(crawler_id, crawl_time)
            
            log_event(EventType.SUCCESS, crawler_id, crawler_name,
                      f"Completed successfully in {duration}s, {headers_added} docs stamped",
                      {
                          'duration': duration,
                          'docCount': doc_count,
                          'headersAdded': headers_added,
                          'outputLines': len(output_lines),
                      })
            return True, None, duration, doc_count
        else:
            error_msg = result.stderr[:1000] if result.stderr else "Unknown error"
            log_event(EventType.ERROR, crawler_id, crawler_name,
                      f"Failed with exit code {result.returncode}",
                      {
                          'exitCode': result.returncode,
                          'error': error_msg,
                          'duration': duration,
                          'stdout': result.stdout[:500] if result.stdout else None,
                      })
            return False, error_msg, duration, 0
            
    except subprocess.TimeoutExpired:
        duration = round(time.time() - start_time, 2)
        log_event(EventType.ERROR, crawler_id, crawler_name,
                  f"Timeout after {duration}s (limit: 600s)")
        return False, "Timeout (10 min)", duration, 0
        
    except Exception as e:
        duration = round(time.time() - start_time, 2)
        error_trace = traceback.format_exc()
        log_event(EventType.ERROR, crawler_id, crawler_name,
                  f"Exception: {str(e)}",
                  {
                      'exception': str(e),
                      'traceback': error_trace,
                      'duration': duration,
                  })
        return False, str(e), duration, 0


def crawl_vendor(crawler_id, force=False):
    """çˆ¬å–æŒ‡å®šå‚å•†"""
    crawler = next((c for c in CRAWLERS if c['id'] == crawler_id), None)
    if not crawler:
        log_system_event(EventType.ERROR, f"Unknown crawler: {crawler_id}")
        return False
    
    status = load_status()
    now = datetime.now()
    now_ts = now.timestamp()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–
    vendor_status = status.get(crawler_id, {})
    last_crawl = vendor_status.get('lastCrawl', 0)
    
    if not force and (now_ts - last_crawl) < crawler['interval']:
        remaining = int(crawler['interval'] - (now_ts - last_crawl))
        hours = remaining // 3600
        minutes = (remaining % 3600) // 60
        log_event(EventType.SKIP, crawler_id, crawler['name'],
                  f"Skipped (next crawl in {hours}h {minutes}m)")
        return True
    
    success, error, duration, doc_count = run_crawler(crawler)
    
    # æ›´æ–°çŠ¶æ€
    status[crawler_id] = {
        'lastCrawl': now_ts,
        'lastCrawlTime': now.isoformat(),
        'success': success,
        'error': error,
        'auto': crawler['auto'],
        'duration': duration,
        'docCount': doc_count,
    }
    save_status(status)
    
    return success


def crawl_all(auto_only=True, force=False):
    """çˆ¬å–æ‰€æœ‰å‚å•†"""
    log_system_event(EventType.START, 
                     f"Batch crawl started (auto_only={auto_only}, force={force})")
    
    print("=" * 60)
    print("LLM Docs Auto Crawler")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    results = []
    total_start = time.time()
    
    for crawler in CRAWLERS:
        if auto_only and not crawler['auto']:
            log_event(EventType.SKIP, crawler['id'], crawler['name'],
                      "Skipped (requires browser)")
            continue
        
        success = crawl_vendor(crawler['id'], force=force)
        results.append((crawler['name'], success))
    
    total_duration = round(time.time() - total_start, 2)
    
    # é‡å»ºæ–‡æ¡£ç´¢å¼•
    log_system_event(EventType.INFO, "Rebuilding docs index...")
    
    try:
        from build_docs_site import build_index
        build_index()
        log_system_event(EventType.SUCCESS, "Docs index rebuilt successfully")
    except Exception as e:
        log_system_event(EventType.ERROR, f"Failed to rebuild index: {e}",
                        {'traceback': traceback.format_exc()})
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for _, s in results if s)
    fail_count = len(results) - success_count
    
    log_system_event(EventType.COMPLETE,
                     f"Batch crawl completed: {success_count} success, {fail_count} failed",
                     {
                         'totalDuration': total_duration,
                         'successCount': success_count,
                         'failCount': fail_count,
                         'results': [{'name': n, 'success': s} for n, s in results],
                     })
    
    print("\n" + "=" * 50)
    print(f"Summary ({total_duration}s)")
    print("=" * 50)
    
    for name, success in results:
        status_icon = "âœ…" if success else "âŒ"
        print(f"  {status_icon} {name}")
    
    return all(s for _, s in results)


def show_status():
    """æ˜¾ç¤ºçˆ¬å–çŠ¶æ€"""
    status = load_status()
    
    print("=" * 60)
    print("Crawl Status")
    print("=" * 60)
    
    for crawler in CRAWLERS:
        vendor_status = status.get(crawler['id'], {})
        last_time = vendor_status.get('lastCrawlTime', 'Never')
        success = vendor_status.get('success', None)
        auto = "ğŸ¤–" if crawler['auto'] else "ğŸ‘¤"
        
        if success is None:
            status_icon = "âšª"
        elif success:
            status_icon = "âœ…"
        else:
            status_icon = "âŒ"
        
        print(f"  {auto} {status_icon} {crawler['name']:<25} Last: {last_time}")


def show_events(count=20):
    """æ˜¾ç¤ºæœ€è¿‘çš„äº‹ä»¶"""
    events = load_events()
    
    print("=" * 60)
    print(f"Recent Events (last {count})")
    print("=" * 60)
    
    for event in events[-count:]:
        ts = event['timestamp'][:19].replace('T', ' ')
        event_type = event['type']
        vendor = event['vendorName']
        msg = event['message']
        
        icon_map = {
            'info': 'â„¹ï¸',
            'success': 'âœ…',
            'warning': 'âš ï¸',
            'error': 'âŒ',
            'start': 'ğŸš€',
            'complete': 'ğŸ‰',
            'skip': 'â­ï¸',
        }
        icon = icon_map.get(event_type, 'â€¢')
        
        print(f"  {ts} {icon} [{vendor}] {msg}")


def clear_events():
    """æ¸…ç©ºäº‹ä»¶æ—¥å¿—"""
    save_events([])
    print("Events cleared.")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='LLM Docs Auto Crawler')
    parser.add_argument('--all', action='store_true', help='Crawl all vendors (including browser-based)')
    parser.add_argument('--force', action='store_true', help='Force crawl even if recently crawled')
    parser.add_argument('--vendor', type=str, help='Crawl specific vendor by ID')
    parser.add_argument('--status', action='store_true', help='Show crawl status')
    parser.add_argument('--list', action='store_true', help='List all crawlers')
    parser.add_argument('--events', type=int, nargs='?', const=20, help='Show recent events')
    parser.add_argument('--clear-events', action='store_true', help='Clear event log')
    
    args = parser.parse_args()
    
    if args.status:
        show_status()
    elif args.list:
        print("Available crawlers:")
        for c in CRAWLERS:
            auto = "ğŸ¤– auto" if c['auto'] else "ğŸ‘¤ manual"
            print(f"  {c['id']:<15} {c['name']:<25} ({auto})")
    elif args.events:
        show_events(args.events)
    elif args.clear_events:
        clear_events()
    elif args.vendor:
        crawl_vendor(args.vendor, force=args.force)
    else:
        crawl_all(auto_only=not args.all, force=args.force)
