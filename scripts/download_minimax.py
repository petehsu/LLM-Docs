#!/usr/bin/env python3
"""
MiniMax API æ–‡æ¡£æ‰¹é‡ä¸‹è½½å·¥å…·

URL è§„å¾‹:
- è‹±æ–‡ç«™: https://platform.minimax.io/docs/{path}.md
- ä¸­æ–‡ç«™: https://platform.minimaxi.com/docs/{path}.md
"""

import sys
import time
import re
import requests
from pathlib import Path

sys.stdout.reconfigure(line_buffering=True)

PROXIES = {"http": "http://127.0.0.1:10808", "https": "http://127.0.0.1:10808"}
USE_PROXY = True

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/markdown,text/plain,*/*',
}

# ä¸¤ä¸ªç«™ç‚¹é…ç½®
SITES = {
    'English': {
        'base_url': 'https://platform.minimax.io',
        'output_dir': 'MiniMax/English',
    },
    'ç®€ä½“ä¸­æ–‡': {
        'base_url': 'https://platform.minimaxi.com',
        'output_dir': 'MiniMax/ç®€ä½“ä¸­æ–‡',
    },
}

# å…¥å£é¡µé¢ï¼Œç”¨äºçˆ¬å–é“¾æ¥
ENTRY_PAGES = [
    '/docs/guides/models-intro',
    '/docs/guides/quickstart',
    '/docs/api-reference/text-intro',
    '/docs/coding-plan/intro',
    '/docs/faq/about-apis',
    '/docs/pricing/overview',
    '/docs/release-notes/models',
    '/docs/solutions/audiobook',
]


def fetch_page(base_url, path):
    """è·å–é¡µé¢ HTML"""
    url = f"{base_url}{path}"
    try:
        resp = requests.get(url, proxies=PROXIES if USE_PROXY else None,
                          headers=HEADERS, timeout=30)
        return resp.text
    except Exception as e:
        print(f"    è·å–é¡µé¢å¤±è´¥: {path} - {e}")
        return ""


def extract_links(html):
    """ä» HTML ä¸­æå–æ–‡æ¡£é“¾æ¥"""
    # åŒ¹é… /docs/ å¼€å¤´çš„é“¾æ¥
    links = re.findall(r'href="(/docs/[^"#]+)"', html)
    
    # è¿‡æ»¤æ‰éæ–‡æ¡£é“¾æ¥
    filtered = []
    exclude_patterns = ['.png', '.ico', '.jpg', '.svg', '.css', '.js', '.woff', '.xml', '?', '_next', '_mintlify']
    for link in links:
        if not any(ext in link for ext in exclude_patterns):
            # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„æ–‡æ¡£è·¯å¾„
            if link.count('/') >= 2:
                filtered.append(link)
    
    return set(filtered)


def get_all_doc_links(base_url):
    """ä»å…¥å£é¡µé¢è·å–æ‰€æœ‰æ–‡æ¡£é“¾æ¥"""
    all_links = set()
    
    for entry in ENTRY_PAGES:
        print(f"  çˆ¬å– {entry}...")
        html = fetch_page(base_url, entry)
        if html:
            links = extract_links(html)
            all_links.update(links)
            print(f"    å‘ç° {len(links)} ä¸ªé“¾æ¥")
        time.sleep(0.5)
    
    return sorted(all_links)


def download_doc(base_url, path, output_dir, skip_existing=True):
    """ä¸‹è½½å•ä¸ªæ–‡æ¡£"""
    url = f"{base_url}{path}.md"
    
    # è¾“å‡ºè·¯å¾„
    rel_path = path.replace('/docs/', '', 1)
    output = f"{output_dir}/{rel_path}.md"
    
    # è·³è¿‡å·²å­˜åœ¨
    if skip_existing and Path(output).exists():
        return True, "è·³è¿‡"
    
    try:
        resp = requests.get(url, proxies=PROXIES if USE_PROXY else None,
                          headers=HEADERS, timeout=30)
        
        if resp.status_code == 404:
            return False, "404"
        
        content = resp.text
        
        # æ£€æŸ¥æ˜¯å¦ä¸º HTML
        if content.strip().startswith('<!DOCTYPE') or content.strip().startswith('<html'):
            return False, "HTML"
        
        # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ•ˆ
        if len(content.strip()) < 10:
            return False, "ç©º"
        
        Path(output).parent.mkdir(parents=True, exist_ok=True)
        with open(output, 'w', encoding='utf-8') as f:
            f.write(content)
        return True, "OK"
    except Exception as e:
        return False, str(e)[:30]


def download_site(lang, config):
    """ä¸‹è½½å•ä¸ªç«™ç‚¹çš„æ‰€æœ‰æ–‡æ¡£"""
    base_url = config['base_url']
    output_dir = config['output_dir']
    
    print(f"\n{'='*50}")
    print(f"ä¸‹è½½ {lang} ç«™ç‚¹: {base_url}")
    print('='*50)
    
    # è·å–é“¾æ¥
    print("\nğŸ“‹ çˆ¬å–æ–‡æ¡£é“¾æ¥...")
    docs = get_all_doc_links(base_url)
    
    if not docs:
        print("æœªæ‰¾åˆ°æ–‡æ¡£")
        return 0, 0
    
    print(f"\n   å…±å‘ç° {len(docs)} ä¸ªå”¯ä¸€é“¾æ¥")
    
    # ä¸‹è½½
    print(f"\nğŸ“š å¼€å§‹ä¸‹è½½...")
    success, skipped, fail = 0, 0, 0
    failed_docs = []
    
    for path in docs:
        name = path.split('/')[-1]
        ok, status = download_doc(base_url, path, output_dir)
        
        if ok:
            if status == "è·³è¿‡":
                skipped += 1
            else:
                success += 1
                print(f"    âœ“ {name}")
        else:
            fail += 1
            failed_docs.append((path, status))
            print(f"    âœ— {name} ({status})")
        
        if status != "è·³è¿‡":
            time.sleep(0.3)
    
    print(f"\nâœ… {lang}: {success} æ–°ä¸‹è½½, {skipped} è·³è¿‡, {fail} å¤±è´¥")
    
    if failed_docs:
        print(f"\nâŒ å¤±è´¥åˆ—è¡¨:")
        for path, status in failed_docs[:5]:
            print(f"    {path} - {status}")
    
    return success + skipped, fail


def main():
    print("=" * 50)
    print("MiniMax API æ–‡æ¡£ä¸‹è½½å·¥å…·")
    print("=" * 50)
    
    total_success, total_fail = 0, 0
    
    for lang, config in SITES.items():
        success, fail = download_site(lang, config)
        total_success += success
        total_fail += fail
    
    print(f"\n{'='*50}")
    print(f"å…¨éƒ¨å®Œæˆ: {total_success} æˆåŠŸ, {total_fail} å¤±è´¥")


if __name__ == "__main__":
    main()
